from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time

# WebDriverの初期化
driver = webdriver.Chrome()

# 店舗リストのページにアクセス
driver.get("https://www.ekiten.jp/g0210/a01217/")  # 例として「札幌市」のマッサージ店リストページ

def extract_store_data(store_url):
    driver.get(store_url)
    time.sleep(2)  # ページ読み込みの待機
    soup = BeautifulSoup(driver.page_source, "html.parser")

    # 店舗名
    name_tag = soup.select_one("h1.ll-a-heading--xxl")
    name = name_tag.text.strip() if name_tag else "店舗名なし"
    print(f"店舗名: {name}")

 
    # 口コミ件数を取得（BeautifulSoupを使用）
    review_count_element = soup.select_one('#pageTop > div.mainFrame > div.mainContainer > div.shopPageHeader > div > div.ll-o-shopHeader__body > div:nth-child(3) > div > div:nth-child(1) > dl > div:nth-child(1) > dd > a > span')
    review_count_text = review_count_element.text.strip() if review_count_element else "口コミ件数なし"
    print(f"口コミ件数: {review_count_text}")

    # 評価ポイント
    rating_element = soup.select_one('span.ll-a-ratingPoint__text')
    rating_text = rating_element.text.strip() if rating_element else "評価ポイントなし"
    print(f"評価ポイント: {rating_text}")

    # 電話番号
    phone_tag = soup.select_one('address a[href^="tel:"]')
    phone = phone_tag.text.strip() if phone_tag else "電話番号なし"
    print(f"電話番号: {phone}")

    # 住所
    address_tag = soup.find('address', class_="ll-u-mb15")
    address = address_tag.text.strip() if address_tag else "住所なし"
    print(f"住所: {address}")

    # 営業時間
    business_hours_tags = soup.select('.ll-m-businessHourList__term, .ll-m-businessHourList__time')
    business_hours = [f"{term.text.strip()}: {time.text.strip()}" for term, time in zip(business_hours_tags[::2], business_hours_tags[1::2])] if business_hours_tags else ["営業時間なし"]
    print(f"営業時間: {', '.join(business_hours)}")

    # 営業時間補足
    extra_hours_tag = soup.find('dd', string=lambda x: "予約受付" in x if x else False)
    extra_hours = extra_hours_tag.text.strip() if extra_hours_tag else "営業時間補足なし"
    print(f"営業時間補足: {extra_hours}")

    # 定休日補足
    extra_closed_tag = soup.find('dd', string=lambda x: any(phrase in x for phrase in ["年末年始", "お盆"]) if x else False)
    extra_closed = extra_closed_tag.text.strip() if extra_closed_tag else "定休日補足なし"
    print(f"定休日補足: {extra_closed}")

    print("-" * 50)

def get_all_store_links():
    # 店舗のリンクをすべて取得
    store_links = []
    stores = driver.find_elements(By.CSS_SELECTOR, "div.ll-m-cassetteContainer a.ll-o-shopCassette__link")
    for store in stores:
        store_links.append(store.get_attribute("href"))
    return store_links

try:
    # ページごとの処理
    while True:
        store_links = get_all_store_links()
        
        # 各店舗のリンクをクリックしてデータを抽出
        for link in store_links:
            extract_store_data(link)
        
        # 次のページがあるか確認
        next_button = driver.find_elements(By.CSS_SELECTOR, ".ll-a-button--nextPage")
        if next_button:
            next_button[0].click()
            time.sleep(2)  # ページ遷移を待つ
        else:
            break  # 次のページがなければ終了

except Exception as e:
    print(f"エラーが発生しました: {e}")

finally:
    # ブラウザを閉じる
    driver.quit()
