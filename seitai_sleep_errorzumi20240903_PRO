import ctypes
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import pandas as pd
import os
import random
import logging

# スリープモードを無効にする関数
ES_CONTINUOUS = 0x80000000
ES_SYSTEM_REQUIRED = 0x00000001
ES_DISPLAY_REQUIRED = 0x00000002

def prevent_sleep():
    ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_SYSTEM_REQUIRED | ES_DISPLAY_REQUIRED)

def allow_sleep():
    ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS)

# ロギングの設定
logging.basicConfig(filename='ekiten_scraper.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# WebDriverの初期化
driver = webdriver.Chrome()

# 取得したデータを保存するリスト
results = []
all_results = []
error_urls = []  # エラーが発生したURLを保存するリスト

def save_to_excel(batch_number):
    try:
        df = pd.DataFrame(results)

        # 出力ファイルパスをユーザーのドキュメントフォルダに設定
        output_directory = os.path.join(os.path.expanduser('~'), 'Documents')

        # バッチごとのバックアップ
        backup_file_path = os.path.join(output_directory, f'ekiten_store_data_batch_{batch_number}.xlsx')
        df.to_excel(backup_file_path, index=False, engine='openpyxl')
        logging.info(f"バッチバックアップ '{backup_file_path}' に保存されました！")

        # バッチデータを全体に追加
        global all_results
        all_results.extend(results)

        # 全体をekiten_store_data.xlsxに保存
        all_df = pd.DataFrame(all_results)
        final_output_file_path = os.path.join(output_directory, 'ekiten_store_data.xlsx')
        all_df.to_excel(final_output_file_path, index=False, engine='openpyxl')
        logging.info(f"すべてのデータを '{final_output_file_path}' に保存しました！")

        results.clear()  # 保存後にバッチ用リストをクリア

    except Exception as e:
        logging.error(f"エクセルへの保存時にエラーが発生しました: {e}")

def extract_store_data(store_url):
    try:
        driver.get(store_url)
        time.sleep(random.uniform(1, 3))  # ページ読み込みの待機（1-3秒のランダムな待機時間）
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # 店舗名
        name_tag = soup.select_one("h1.ll-a-heading--xxl")
        name = name_tag.text.strip() if name_tag else "店舗名なし"

        # 電話番号
        phone_tag = soup.select_one('address a[href^="tel:"]')
        phone = phone_tag.text.strip() if phone_tag else "電話番号なし"

        # 住所
        address_tag = soup.find('address', class_="ll-u-mb15")
        address = address_tag.text.strip() if address_tag else "住所なし"

        # 営業時間
        business_hours_tags = soup.select('.ll-m-businessHourList__term, .ll-m-businessHourList__time')
        business_hours = [f"{term.text.strip()}: {time.text.strip()}" for term, time in zip(business_hours_tags[::2], business_hours_tags[1::2])] if business_hours_tags else ["営業時間なし"]
        business_hours_str = ', '.join(business_hours)

        # 評価ポイント
        rating_element = soup.select_one('span.ll-a-ratingPoint__text')
        rating_text = rating_element.text.strip() if rating_element else "評価ポイントなし"

        # 口コミ件数
        review_count_text = "口コミ件数なし"  # デフォルト値
        for dt in soup.find_all('dt'):
            try:
                if '口コミ' in dt.get_text():
                    review_count_text = dt.find_next_sibling('dd').find('span', class_='ll-a-textLink__label').get_text()
                    break
            except Exception as inner_e:
                logging.warning(f"口コミ数の抽出時にエラーが発生しました（{store_url}）: {inner_e}")

        # データをリストとして保存
        results.append({
            "店舗名": name,
            "電話番号": phone,
            "住所": address,
            "営業時間": business_hours_str,
            "評価": rating_text,
            "口コミ数": review_count_text
        })

        logging.info(f"データ取得成功: {name} - {phone} - {address}")

    except Exception as e:
        logging.error(f"データ抽出時にエラーが発生しました（{store_url}）: {e}")
        error_urls.append(store_url)  # エラーが発生したURLを保存

def get_all_store_links():
    try:
        store_links = []
        stores = driver.find_elements(By.CSS_SELECTOR, "div.ll-m-cassetteContainer a.ll-o-shopCassette__link")
        for store in stores:
            store_links.append(store.get_attribute("href"))
        return store_links
    except Exception as e:
        logging.error(f"店舗リンクの取得時にエラーが発生しました: {e}")
        return []

try:
    prevent_sleep()  # スリープモードを無効にする

    print("URLを入力してエンターを押してください。入力が終わったら、空行（何も入力せずエンター）を入力してください:")

    urls = []
    while True:
        url = input().strip()
        if url:
            urls.append(url)
        else:
            break

    batch_number = 1
    for url in urls:
        try:
            driver.get(url)

            while True:
                store_links = get_all_store_links()

                if not store_links:
                    logging.warning(f"リンクが見つかりませんでした（{url}）")
                    break

                for link in store_links:
                    extract_store_data(link)

                    if len(results) >= 100:
                        save_to_excel(batch_number)
                        batch_number += 1

                next_button = driver.find_elements(By.CSS_SELECTOR, ".ll-a-button--nextPage")
                if next_button:
                    try:
                        next_button[0].click()
                        time.sleep(random.uniform(2, 4))  # ページ遷移を待つ（2-4秒のランダムな待機時間）
                    except Exception as e:
                        logging.error(f"次のページへの移動時にエラーが発生しました（{url}）: {e}")
                        error_urls.append(url)  # エラーが発生したURLを保存
                        break
                else:
                    break

        except Exception as e:
            logging.error(f"URL処理中にエラーが発生しました（{url}）: {e}")
            error_urls.append(url)  # エラーが発生したURLを保存
            continue  # エラーが発生した場合、次のURLにスキップ

    if results:
        save_to_excel(batch_number)

    # 最後にエラーが発生したURLをターミナルで表示
    if error_urls:
        print("エラーが発生したURL:")
        for error_url in error_urls:
            print(error_url)
        logging.info("エラーが発生したURL: " + ', '.join(error_urls))

except Exception as e:
    logging.critical(f"メインプロセス中にエラーが発生しました: {e}")

finally:
    allow_sleep()  # スリープモードを再度有効にする
    driver.quit()
    logging.info("ブラウザを正常に閉じました。")
    # 処理が終了したらエラーが発生したURLをターミナルに表示
    if error_urls:
        print("エラーが発生したURL:")
        for error_url in error_urls:
            print(error_url)
